{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84916569-3e1c-4cf8-995d-d818284ec7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|              review|label|               words|         rawFeatures|            features|label_indexed|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|\"\"\" Så som i himm...|    0|[\"\"\", Så, som, i,...|(65536,[1076,1981...|(65536,[1076,1981...|          0.0|[-1803.6744929212...|[0.99972107169651...|       0.0|\n",
      "|\"\"\"A lot of the f...|    0|[\"\"\"A, lot, of, t...|(65536,[1578,1981...|(65536,[1578,1981...|          0.0|[-360.48884114161...|[0.99999984816176...|       0.0|\n",
      "|\"\"\"A wrong-doer i...|    0|[\"\"\"A, wrong-doer...|(65536,[1714,5616...|(65536,[1714,5616...|          0.0|[-355.76194808462...|[0.99999991626398...|       0.0|\n",
      "|\"\"\"Ally McBeal\"\" ...|    0|[\"\"\"Ally, McBeal\"...|(65536,[4331,2871...|(65536,[4331,2871...|          0.0|[-231.15976930238...|[5.12746013846417...|       1.0|\n",
      "|\"\"\"And the time c...|    0|[\"\"\"And, the, tim...|(65536,[308,7723,...|(65536,[308,7723,...|          0.0|[-882.25811965163...|[0.99999999999997...|       0.0|\n",
      "|\"\"\"Ardh Satya\"\" i...|    0|[\"\"\"Ardh, Satya\"\"...|(65536,[3085,7194...|(65536,[3085,7194...|          0.0|[-454.30953674269...|[1.64624952542340...|       1.0|\n",
      "|\"\"\"Balance of Ter...|    0|[\"\"\"Balance, of, ...|(65536,[3085,6042...|(65536,[3085,6042...|          0.0|[-582.39331238936...|[8.76946610674362...|       1.0|\n",
      "|\"\"\"Cinderella\"\" i...|    0|[\"\"\"Cinderella\"\",...|(65536,[1880,1981...|(65536,[1880,1981...|          0.0|[-757.16546790044...|[2.19544767812960...|       1.0|\n",
      "|\"\"\"Coconut Fred's...|    0|[\"\"\"Coconut, Fred...|(65536,[1880,1128...|(65536,[1880,1128...|          0.0|[-657.39856951634...|[0.99999999999797...|       0.0|\n",
      "|\"\"\"Crossfire\"\" is...|    0|[\"\"\"Crossfire\"\", ...|(65536,[74,482,29...|(65536,[74,482,29...|          0.0|[-684.68378060995...|[3.34381176208535...|       1.0|\n",
      "|\"\"\"Crossfire\"\" is...|    0|[\"\"\"Crossfire\"\", ...|(65536,[1981,2294...|(65536,[1981,2294...|          0.0|[-1868.7860689263...|[0.99991917052177...|       0.0|\n",
      "|\"\"\"Down Periscope...|    0|[\"\"\"Down, Perisco...|(65536,[1714,4629...|(65536,[1714,4629...|          0.0|[-450.74904895260...|[0.58798827822782...|       0.0|\n",
      "|\"\"\"Fate\"\" leads W...|    0|[\"\"\"Fate\"\", leads...|(65536,[1714,5226...|(65536,[1714,5226...|          0.0|[-589.14050069537...|[0.01272010904971...|       1.0|\n",
      "|\"\"\"Fungicide\"\" is...|    0|[\"\"\"Fungicide\"\", ...|(65536,[4616,3035...|(65536,[4616,3035...|          0.0|[-214.98475001154...|[0.95618925878873...|       0.0|\n",
      "|\"\"\"Gypsy\"\" is pos...|    0|[\"\"\"Gypsy\"\", is, ...|(65536,[7194,2060...|(65536,[7194,2060...|          0.0|[-153.91990646042...|[0.42721698190716...|       1.0|\n",
      "|\"\"\"Homeward Bound...|    0|[\"\"\"Homeward, Bou...|(65536,[3785,4888...|(65536,[3785,4888...|          0.0|[-934.57824276006...|[0.99894672378725...|       0.0|\n",
      "|\"\"\"I like cheap p...|    0|[\"\"\"I, like, chea...|(65536,[1981,5090...|(65536,[1981,5090...|          0.0|[-986.34713603714...|[3.32553362633580...|       1.0|\n",
      "|\"\"\"I presume you ...|    0|[\"\"\"I, presume, y...|(65536,[5090,9712...|(65536,[5090,9712...|          0.0|[-789.76716754571...|[0.99999801674770...|       0.0|\n",
      "|\"\"\"Intensive Care...|    0|[\"\"\"Intensive, Ca...|(65536,[1981,3085...|(65536,[1981,3085...|          0.0|[-1425.7868189652...|[0.99999999999987...|       0.0|\n",
      "|\"\"\"Lies\"\" tells a...|    0|[\"\"\"Lies\"\", tells...|(65536,[2306,2766...|(65536,[2306,2766...|          0.0|[-2459.4872421682...|[1.0,4.2982566377...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy = 80.12%\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------+\n",
      "|              review|               words|         rawFeatures|            features|       rawPrediction|         probability|prediction|sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------+\n",
      "|Surely one the Fr...|[Surely, one, the...|(65536,[3785,4319...|(65536,[3785,4319...|[-55.488421746796...|[0.67986159794411...|       0.0| negative|\n",
      "|\"Screened this mo...|[\"Screened, this,...|(65536,[600,1880,...|(65536,[600,1880,...|[-38.732329723237...|[0.67303716419216...|       0.0| negative|\n",
      "|\"I'm not a sports...|[\"I'm, not, a, sp...|(65536,[1502,1714...|(65536,[1502,1714...|[-90.926827360329...|[0.32706984843608...|       1.0| positive|\n",
      "|This movie has it...|[This, movie, has...|(65536,[1714,2702...|(65536,[1714,2702...|[-240.41898740594...|[0.25615888169120...|       1.0| positive|\n",
      "|\"This is the defi...|[\"This, is, the, ...|(65536,[1857,5086...|(65536,[1857,5086...|[-46.574792979416...|[0.59035510052069...|       0.0| negative|\n",
      "|\"Has the drama, s...|[\"Has, the, drama...|(65536,[807,1126,...|(65536,[807,1126,...|[-190.01895781953...|[0.52485013706045...|       0.0| negative|\n",
      "|This is a prime e...|[This, is, a, pri...|(65536,[171,401,7...|(65536,[171,401,7...|[-215.71136815193...|[0.13554295339579...|       1.0| positive|\n",
      "|Big Fat Liar is a...|[Big, Fat, Liar, ...|(65536,[1113,1832...|(65536,[1113,1832...|[-83.356374123566...|[0.72892538208724...|       0.0| negative|\n",
      "|A remake of Aleja...|[A, remake, of, A...|(65536,[1083,1944...|(65536,[1083,1944...|[-151.64813307487...|[0.38038526790162...|       1.0| positive|\n",
      "|\"If you are one o...|[\"If, you, are, o...|(65536,[2697,5923...|(65536,[2697,5923...|[-54.227188177405...|[0.62578635810695...|       0.0| negative|\n",
      "|Simon Pegg plays ...|[Simon, Pegg, pla...|(65536,[1880,2783...|(65536,[1880,2783...|[-144.23355981413...|[0.48107708003372...|       1.0| positive|\n",
      "|\"Even from the ve...|[\"Even, from, the...|(65536,[95,835,10...|(65536,[95,835,10...|[-119.53411526666...|[0.47180109334393...|       1.0| positive|\n",
      "|\"Usually, I start...|[\"Usually,, I, st...|(65536,[401,835,1...|(65536,[401,835,1...|[-175.06125791749...|[0.55448698247882...|       0.0| negative|\n",
      "|Thats not saying ...|[Thats, not, sayi...|(65536,[161,308,6...|(65536,[161,308,6...|[-171.50460638767...|[0.55890464813194...|       0.0| negative|\n",
      "|\"The acting- fant...|[\"The, acting-, f...|(65536,[120,835,8...|(65536,[120,835,8...|[-200.27172658248...|[0.64406557872134...|       0.0| negative|\n",
      "|\"My abiding love ...|[\"My, abiding, lo...|(65536,[184,12567...|(65536,[184,12567...|[-25.799153696435...|[0.48086922082538...|       1.0| positive|\n",
      "|\"I really enjoy t...|[\"I, really, enjo...|(65536,[388,835,1...|(65536,[388,835,1...|[-235.60576080742...|[0.32913529144501...|       1.0| positive|\n",
      "|I confess to have...|[I, confess, to, ...|(65536,[172,788,1...|(65536,[172,788,1...|[-215.22425906467...|[0.70540673496583...|       0.0| negative|\n",
      "|There is a lot of...|[There, is, a, lo...|(65536,[822,1657,...|(65536,[822,1657,...|[-266.11928337680...|[0.29480011633634...|       1.0| positive|\n",
      "|I absolutely ador...|[I, absolutely, a...|(65536,[1106,1511...|(65536,[1106,1511...|[-178.08799216385...|[0.50702126624476...|       0.0| negative|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------+\n",
      "\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+-------------+--------------------+----------+\n",
      "|              review|label|               words|         rawFeatures|            features|label_indexed| vectorized_features|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+-------------+--------------------+----------+\n",
      "|\"\"\"A bored televi...|    0|[\"\"\"A, bored, tel...|(65536,[3085,4703...|(65536,[3085,4703...|          0.0|(65536,[3085,4703...|         0|\n",
      "|\"\"\"A trio of trea...|    0|[\"\"\"A, trio, of, ...|(65536,[1064,1981...|(65536,[1064,1981...|          0.0|(65536,[1064,1981...|         0|\n",
      "|\"\"\"Ah Ritchie's m...|    0|[\"\"\"Ah, Ritchie's...|(65536,[9712,1127...|(65536,[9712,1127...|          0.0|(65536,[9712,1127...|         0|\n",
      "|\"\"\"American Night...|    0|[\"\"\"American, Nig...|(65536,[5233,1153...|(65536,[5233,1153...|          0.0|(65536,[5233,1153...|         0|\n",
      "|\"\"\"Back of Beyond...|    0|[\"\"\"Back, of, Bey...|(65536,[3085,3691...|(65536,[3085,3691...|          0.0|(65536,[3085,3691...|         0|\n",
      "|\"\"\"Bell Book and ...|    0|[\"\"\"Bell, Book, a...|(65536,[120,1603,...|(65536,[120,1603,...|          0.0|(65536,[120,1603,...|         0|\n",
      "|\"\"\"Crossfire\"\" is...|    0|[\"\"\"Crossfire\"\", ...|(65536,[3059,1603...|(65536,[3059,1603...|          0.0|(65536,[3059,1603...|         0|\n",
      "|\"\"\"Direct-to-vide...|    0|[\"\"\"Direct-to-vid...|(65536,[2803,5471...|(65536,[2803,5471...|          0.0|(65536,[2803,5471...|         0|\n",
      "|\"\"\"Escape from He...|    0|[\"\"\"Escape, from,...|(65536,[342,1401,...|(65536,[342,1401,...|          0.0|(65536,[342,1401,...|         0|\n",
      "|\"\"\"Feast of All S...|    0|[\"\"\"Feast, of, Al...|(65536,[12,505,16...|(65536,[12,505,16...|          0.0|(65536,[12,505,16...|         0|\n",
      "|\"\"\"Her Cardboard ...|    0|[\"\"\"Her, Cardboar...|(65536,[2595,5381...|(65536,[2595,5381...|          0.0|(65536,[5381,6405...|         0|\n",
      "|\"\"\"Hero and the T...|    0|[\"\"\"Hero, and, th...|(65536,[17118,223...|(65536,[17118,223...|          0.0|(65536,[17118,223...|         0|\n",
      "|\"\"\"Hey Babu Riba\"...|    0|[\"\"\"Hey, Babu, Ri...|(65536,[4851,1729...|(65536,[4851,1729...|          0.0|(65536,[4851,1729...|         0|\n",
      "|\"\"\"Indian burial ...|    0|[\"\"\"Indian, buria...|(65536,[581,7503,...|(65536,[581,7503,...|          0.0|(65536,[581,7503,...|         0|\n",
      "|            \"\"\"Laugh|    0|          [\"\"\"Laugh]|(65536,[9516],[1.0])|(65536,[9516],[5....|          0.0|(65536,[9516],[5....|         0|\n",
      "|\"\"\"Mararía\"\" real...|    0|[\"\"\"Mararía\"\", re...|(65536,[1981,1186...|(65536,[1981,1186...|          0.0|(65536,[1981,1186...|         0|\n",
      "|\"\"\"Milo\"\" is yet ...|    0|[\"\"\"Milo\"\", is, y...|(65536,[2306,5451...|(65536,[2306,5451...|          0.0|(65536,[2306,5451...|         0|\n",
      "|\"\"\"Mr. Harvey Lig...|    0|[\"\"\"Mr., Harvey, ...|(65536,[1500,3085...|(65536,[1500,3085...|          0.0|(65536,[1500,3085...|         0|\n",
      "|\"\"\"Nada\"\" was the...|    0|[\"\"\"Nada\"\", was, ...|(65536,[17982,208...|(65536,[17982,208...|          0.0|(65536,[17982,275...|         0|\n",
      "|\"\"\"Night of the L...|    0|[\"\"\"Night, of, th...|(65536,[1143,4114...|(65536,[1143,4114...|          0.0|(65536,[1143,4114...|         0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Silhouette Score = 0.51\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, when\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF,IDF,StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# create a SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "# load data\n",
    "\n",
    "data = spark.read.csv(\"./IMDB Dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# prepare data\n",
    "data = data.select(\"review\", \"sentiment\")\n",
    "data = data.withColumn(\"label\", when(col(\"sentiment\") == \"positive\", 1).otherwise(0))\n",
    "\n",
    "# split data into training and test sets\n",
    "train, test = data.randomSplit([0.6, 0.4], seed=42)\n",
    "\n",
    "# extract features\n",
    "hashing_tf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_indexed\").fit(data)\n",
    "\n",
    "# tokenize training data\n",
    "train_words = train.select(\"review\", \"label\").withColumn(\"words\", split(col(\"review\"), \"\\s+\"))\n",
    "\n",
    "# transform training data|\n",
    "train_raw_features = hashing_tf.transform(train_words)\n",
    "train_features = idf.fit(train_raw_features).transform(train_raw_features)\n",
    "train_features = label_indexer.transform(train_features)\n",
    "\n",
    "# train Naive Bayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"label_indexed\")\n",
    "model = nb.fit(train_features)\n",
    "\n",
    "# tokenize test data\n",
    "test_words = test.select(\"review\", \"label\").withColumn(\"words\", split(col(\"review\"), \"\\s+\"))\n",
    "\n",
    "# transform test data\n",
    "test_raw_features = hashing_tf.transform(test_words)\n",
    "test_features = idf.fit(test_raw_features).transform(test_raw_features)\n",
    "test_features = label_indexer.transform(test_features)\n",
    "\n",
    "# make predictions\n",
    "predictions = model.transform(test_features)\n",
    "predictions.show()\n",
    "\n",
    "# evaluate model performance\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_indexed\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# create a DataFrame with the reviews to be predicted\n",
    "new_data = spark.read.csv(\"Predict Dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# transform new data\n",
    "new_data_words = new_data.select(\"review\").withColumn(\"words\", split(col(\"review\"), \"\\s+\"))\n",
    "new_data_raw_features = hashing_tf.transform(new_data_words)\n",
    "new_data_features = idf.fit(new_data_raw_features).transform(new_data_raw_features)\n",
    "\n",
    "# add sentiment analysis\n",
    "new_data_predictions = model.transform(new_data_features)\n",
    "new_data_predictions = new_data_predictions.withColumn(\"sentiment\", when(col(\"prediction\") == 1, \"positive\").otherwise(\"negative\"))\n",
    "\n",
    "# show predictions\n",
    "new_data_predictions.show()\n",
    "\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "\n",
    "# vectorize the text data\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"vectorized_features\")\n",
    "train_vectorized = assembler.transform(train_features)\n",
    "\n",
    "# train k-means model\n",
    "kmeans = KMeans(k=2, seed=1)\n",
    "model_kmeans = kmeans.fit(train_vectorized)\n",
    "\n",
    "# assign cluster labels to the data\n",
    "train_clustered = model_kmeans.transform(train_vectorized)\n",
    "\n",
    "# display the cluster assignments\n",
    "train_clustered.show()\n",
    "\n",
    "# evaluate clustering performance\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"vectorized_features\")\n",
    "silhouette = evaluator.evaluate(train_clustered)\n",
    "print(\"Silhouette Score = {:.2f}\".format(silhouette))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
